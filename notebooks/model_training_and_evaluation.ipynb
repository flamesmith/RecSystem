{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recommendation System — Train / Inference Evaluation\n",
        "\n",
        "This notebook tests all models in the new separated train-once / infer-many framework:\n",
        "\n",
        "1. Load data and sample a random cutoff day\n",
        "2. **Training phase** — fit all models on pre-cutoff data and save artifacts to `models/`\n",
        "3. **Inference phase** — reload models from disk (simulates a fresh server / daily job)\n",
        "4. **Evaluate** — compare all models offline using post-cutoff interactions as ground truth\n",
        "5. **Results** — summary table and metric chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Resolve project root regardless of kernel working directory\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "else:\n",
        "    PROJECT_ROOT = os.getcwd()\n",
        "\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "# ── Model classes ──────────────────────────────────────────────────────────\n",
        "from functions.ials_implicit_package import IALSImplicitRecommender\n",
        "from functions.interaction_matrix import InteractionMatrixBuilder\n",
        "from functions.item_item_knn import ItemItemRecommenderKnn\n",
        "from functions.model_store import list_saved_models, load_model, save_model\n",
        "from functions.rs_baseline_models import (\n",
        "    CooccurrenceRecommender,\n",
        "    PopularityRecommender,\n",
        "    TrendingRecommender,\n",
        ")\n",
        "from functions import rs_evaluation as rsev\n",
        "\n",
        "DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'Home_and_Kitchen_filtered.csv')\n",
        "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
        "\n",
        "print('Project root :', PROJECT_ROOT)\n",
        "print('Data path    :', DATA_PATH)\n",
        "print('Models dir   :', MODELS_DIR)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/vachemacbook/tfod/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Project root : /Users/vachemacbook/Desktop/RecSystem/RecSystem\n",
            "Data path    : /Users/vachemacbook/Desktop/RecSystem/RecSystem/data/Home_and_Kitchen_filtered.csv\n",
            "Models dir   : /Users/vachemacbook/Desktop/RecSystem/RecSystem/models\n"
          ]
        }
      ],
      "id": "dc96f844"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data & Sample Cutoff Day"
      ],
      "id": "83471803"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.sort_values('unixReviewTime').reset_index(drop=True)\n",
        "df['review_dt'] = pd.to_datetime(df['unixReviewTime'], unit='s')\n",
        "\n",
        "print(f'Loaded {len(df):,} interactions')\n",
        "print(f'Time span : {df[\"review_dt\"].min().date()}  →  {df[\"review_dt\"].max().date()}')\n",
        "print(f'Unique users : {df[\"reviewerID\"].nunique():,}')\n",
        "print(f'Unique items : {df[\"asin\"].nunique():,}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/var/folders/wp/mbxlfymj4fg9dwsmpnqs92x80000gn/T/ipykernel_81928/1883381289.py:1: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(DATA_PATH)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 6,898,955 interactions\n",
            "Time span : 2000-05-02  →  2018-10-04\n",
            "Unique users : 777,242\n",
            "Unique items : 189,172\n"
          ]
        }
      ],
      "id": "453f21dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Sample a random cutoff day in the middle of the timeline ───────────────\n",
        "# We work in whole days so the boundary is clean and easy to reproduce.\n",
        "\n",
        "def sample_cutoff_day(df, random_state=42, lower_q=0.3, upper_q=0.7):\n",
        "    \"\"\"Pick a random day between the given quantiles of the unix timestamp distribution.\"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    ts = df['unixReviewTime'].to_numpy()\n",
        "    low, high = np.quantile(ts, lower_q), np.quantile(ts, upper_q)\n",
        "    days = np.unique((ts[(ts >= low) & (ts <= high)] // 86_400) * 86_400)\n",
        "    cutoff_unix = int(rng.choice(days))\n",
        "    return cutoff_unix, pd.to_datetime(cutoff_unix, unit='s')\n",
        "\n",
        "\n",
        "CUTOFF_UNIX, CUTOFF_TS = sample_cutoff_day(df)\n",
        "\n",
        "df_train = df[df['unixReviewTime'] < CUTOFF_UNIX].copy()\n",
        "df_test  = df[df['unixReviewTime'] > CUTOFF_UNIX].copy()\n",
        "\n",
        "train_users = set(df_train['reviewerID'].unique())\n",
        "test_users  = set(df_test['reviewerID'].unique())\n",
        "ALL_EVAL_USERS = sorted(train_users & test_users)\n",
        "\n",
        "# ── Subsample eval users for fast iteration ────────────────────────────────\n",
        "MAX_EVAL_USERS = 2_000\n",
        "rng_eval = np.random.default_rng(0)\n",
        "EVAL_USERS = (\n",
        "    list(rng_eval.choice(ALL_EVAL_USERS, size=MAX_EVAL_USERS, replace=False))\n",
        "    if len(ALL_EVAL_USERS) > MAX_EVAL_USERS\n",
        "    else ALL_EVAL_USERS\n",
        ")\n",
        "\n",
        "print(f'Cutoff day       : {CUTOFF_TS.date()}  (unix={CUTOFF_UNIX})')\n",
        "print(f'Train interactions: {len(df_train):,}')\n",
        "print(f'Test  interactions: {len(df_test):,}')\n",
        "print(f'Eval users (full) : {len(ALL_EVAL_USERS):,}')\n",
        "print(f'Eval users (sample): {len(EVAL_USERS):,}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cutoff day       : 2015-05-28  (unix=1432771200)\n",
            "Train interactions: 2,251,903\n",
            "Test  interactions: 4,643,855\n",
            "Eval users (full) : 423,925\n",
            "Eval users (sample): 2,000\n"
          ]
        }
      ],
      "id": "1b23b1e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Training Phase\n",
        "\n",
        "Fit every model on pre-cutoff data, then persist to `models/`.  \n",
        "In production this entire section runs once per day as a batch job."
      ],
      "id": "0c232a51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Hyperparameters (lightweight defaults for quick testing) ───────────────\n",
        "KNN_K          = 30\n",
        "KNN_MIN_USERS  = 10\n",
        "IALS_FACTORS   = 32\n",
        "IALS_EPOCHS    = 10\n",
        "IALS_ALPHA     = 40\n",
        "MATRIX_MIN_USERS = 10"
      ],
      "execution_count": 6,
      "outputs": [],
      "id": "2bd057ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── 1. Popularity ──────────────────────────────────────────────────────────\n",
        "t = time.time()\n",
        "pop_model = PopularityRecommender().fit(df_train, CUTOFF_UNIX)\n",
        "save_model(pop_model, 'popularity', models_dir=MODELS_DIR)\n",
        "print(f'Popularity trained and saved  ({time.time()-t:.1f}s)')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved 'popularity' → /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/popularity.joblib\n",
            "Popularity trained and saved  (2.5s)\n"
          ]
        }
      ],
      "id": "5096f078"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── 2. Trending ────────────────────────────────────────────────────────────\n",
        "t = time.time()\n",
        "trend_model = TrendingRecommender(n_days=7).fit(df_train, CUTOFF_UNIX)\n",
        "save_model(trend_model, 'trending', models_dir=MODELS_DIR)\n",
        "print(f'Trending trained and saved  ({time.time()-t:.1f}s)')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved 'trending' → /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/trending.joblib\n",
            "Trending trained and saved  (3.3s)\n"
          ]
        }
      ],
      "id": "51ab53a6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── 3. Co-occurrence ───────────────────────────────────────────────────────\n",
        "t = time.time()\n",
        "cooc_model = CooccurrenceRecommender().fit(df_train, CUTOFF_UNIX)\n",
        "save_model(cooc_model, 'cooccurrence', models_dir=MODELS_DIR)\n",
        "print(f'Co-occurrence trained and saved  ({time.time()-t:.1f}s)')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved 'cooccurrence' → /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/cooccurrence.joblib\n",
            "Co-occurrence trained and saved  (8.1s)\n"
          ]
        }
      ],
      "id": "95298bef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── 4. Item-Item kNN ───────────────────────────────────────────────────────\n",
        "t = time.time()\n",
        "knn_model = ItemItemRecommenderKnn(k=KNN_K, min_users=KNN_MIN_USERS, shrinkage=10).fit(\n",
        "    df_train, cutoff_time=CUTOFF_UNIX\n",
        ")\n",
        "save_model(knn_model, 'item_knn', models_dir=MODELS_DIR)\n",
        "print(f'Item-Item kNN trained and saved  ({time.time()-t:.1f}s)')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved 'item_knn' → /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/item_knn.joblib\n",
            "Item-Item kNN trained and saved  (13.7s)\n"
          ]
        }
      ],
      "id": "e5f87c6a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── 5. IALS ────────────────────────────────────────────────────────────────\n",
        "t = time.time()\n",
        "builder = InteractionMatrixBuilder(min_users=MATRIX_MIN_USERS)\n",
        "train_matrix = builder.build(df_train)\n",
        "print(f'Interaction matrix: {train_matrix.shape[0]:,} users × {train_matrix.shape[1]:,} items')\n",
        "\n",
        "ials_model = IALSImplicitRecommender(\n",
        "    factors=IALS_FACTORS, regularization=0.01, alpha=IALS_ALPHA,\n",
        "    epochs=IALS_EPOCHS, use_gpu=False\n",
        ").fit(train_matrix, builder.item_map, builder.items, builder.user_map, builder.users)\n",
        "save_model(ials_model, 'ials', models_dir=MODELS_DIR)\n",
        "print(f'IALS trained and saved  ({time.time()-t:.1f}s)')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interaction matrix: 462,223 users × 41,001 items\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/vachemacbook/tfod/lib/python3.8/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 6 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
            "  check_blas_config()\n",
            "100%|██████████| 10/10 [01:06<00:00,  6.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Saved 'ials' → /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/ials.joblib\n",
            "IALS trained and saved  (78.8s)\n"
          ]
        }
      ],
      "id": "2d09b205"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('Saved artifacts in models/:')\n",
        "for name in list_saved_models(MODELS_DIR):\n",
        "    print(f'  {name}.joblib')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved artifacts in models/:\n",
            "  cooccurrence.joblib\n",
            "  ials.joblib\n",
            "  item_knn.joblib\n",
            "  popularity.joblib\n",
            "  trending.joblib\n"
          ]
        }
      ],
      "id": "622babe4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Inference Phase\n",
        "\n",
        "Load each model from disk — this is what a production server does once at startup.  \n",
        "From this point on, no raw data is needed; every `recommend()` call is a fast in-memory lookup."
      ],
      "id": "d8bb5fd7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load all five models from their joblib artifacts\n",
        "pop_inf   = load_model(PopularityRecommender,   'popularity',   models_dir=MODELS_DIR)\n",
        "trend_inf = load_model(TrendingRecommender,     'trending',     models_dir=MODELS_DIR)\n",
        "cooc_inf  = load_model(CooccurrenceRecommender, 'cooccurrence', models_dir=MODELS_DIR)\n",
        "knn_inf   = load_model(ItemItemRecommenderKnn,  'item_knn',     models_dir=MODELS_DIR)\n",
        "ials_inf  = load_model(IALSImplicitRecommender, 'ials',         models_dir=MODELS_DIR)\n",
        "\n",
        "print('All models loaded. Ready to serve.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 'popularity' from /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/popularity.joblib\n",
            "Loaded 'trending' from /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/trending.joblib\n",
            "Loaded 'cooccurrence' from /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/cooccurrence.joblib\n",
            "Loaded 'item_knn' from /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/item_knn.joblib\n",
            "Loaded 'ials' from /Users/vachemacbook/Desktop/RecSystem/RecSystem/models/ials.joblib\n",
            "All models loaded. Ready to serve.\n"
          ]
        }
      ],
      "id": "18a4f094"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick smoke-test: generate recommendations for one known user\n",
        "demo_user = EVAL_USERS[0]\n",
        "demo_hist = df_train[df_train['reviewerID'] == demo_user]['asin'].tolist()\n",
        "\n",
        "print(f'Demo user : {demo_user}')\n",
        "print(f'History   : {len(demo_hist)} items')\n",
        "print()\n",
        "print('Popularity   :', pop_inf.recommend(demo_hist, n=5))\n",
        "print('Trending     :', trend_inf.recommend(demo_hist, n=5))\n",
        "print('Co-occurrence:', cooc_inf.recommend(demo_hist, n=5))\n",
        "print('Item-kNN     :', knn_inf.recommend(demo_hist, top_n=5))\n",
        "print('IALS         :', ials_inf.recommend(demo_user, demo_hist, top_n=5))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Demo user : A1WMZ7LN1AH4XW\n",
            "History   : 6 items\n",
            "\n",
            "Popularity   : ['B0015TMHSI', 'B000ZK5UT6', 'B000YGEVMI', 'B0017XHSAE', 'B00902X68W']\n",
            "Trending     : ['B00GRAEZNK', 'B00SV0EF4S', 'B00PRDN64M', 'B00R3Z49G6', 'B00T1Q2KRU']\n",
            "Co-occurrence: ['B000WHE5PC', 'B0014CX87U', 'B00WMK5X08', 'B000RO0TUU', 'B001LGWMHG']\n",
            "Item-kNN     : ['B003K251HW', 'B00JTSEENS', 'B0090E4HD8', 'B00CZ2OJJO', 'B005OCZT1Y']\n",
            "IALS         : ['B003A2IDMC', 'B0014X7ARI', 'B00007E7RY', 'B000ZK5UT6', 'B000H0SDD4']\n"
          ]
        }
      ],
      "id": "9b643a56"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Offline Evaluation\n",
        "\n",
        "**Protocol:** train on pre-cutoff interactions → recommend → check whether recommended items\n",
        "appear in each user's post-cutoff interactions.  \n",
        "Metrics computed per user then averaged: Precision@K, Recall@K, Hit@K, NDCG@K."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pre-compute user histories and future interactions (done once, not per model)\n",
        "user_history_map = (\n",
        "    df_train.groupby('reviewerID')['asin']\n",
        "    .apply(list)\n",
        "    .to_dict()\n",
        ")\n",
        "user_future_map = (\n",
        "    df_test.groupby('reviewerID')['asin']\n",
        "    .apply(lambda s: list(set(s)))\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "print(f'Users with history: {len(user_history_map):,}')\n",
        "print(f'Users with future interactions: {len(user_future_map):,}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Users with history: 484,945\n",
            "Users with future interactions: 716,201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_model(model_name, recommend_fn, eval_users, user_history_map, user_future_map, k=10):\n",
        "    \"\"\"Evaluate a model over a set of users, returning averaged metrics.\n",
        "\n",
        "    recommend_fn signature: (user_id: str, history: list, k: int) -> list[str]\n",
        "    \"\"\"\n",
        "    precs, recs, hits, ndcgs = [], [], [], []\n",
        "\n",
        "    for uid in eval_users:\n",
        "        hist   = user_history_map.get(uid, [])\n",
        "        future = user_future_map.get(uid, [])\n",
        "        if not future:\n",
        "            continue\n",
        "\n",
        "        recs_u = recommend_fn(uid, hist, k)\n",
        "\n",
        "        precs.append(rsev.calculate_precision_at_k(recs_u, future, k))\n",
        "        recs.append(rsev.calculate_recall_at_k(recs_u, future, k))\n",
        "        hits.append(rsev.calculate_hit_at_k(recs_u, future, k))\n",
        "        ndcgs.append(rsev.calculate_ndcg_at_k(recs_u, future, k))\n",
        "\n",
        "    avg = lambda xs: float(np.mean(xs)) if xs else 0.0\n",
        "    return {\n",
        "        'model':       model_name,\n",
        "        'k':           k,\n",
        "        'precision@k': avg(precs),\n",
        "        'recall@k':    avg(recs),\n",
        "        'hit@k':       avg(hits),\n",
        "        'ndcg@k':      avg(ndcgs),\n",
        "        'n_users':     len(precs),\n",
        "    }"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Wrap each loaded model into the unified (user_id, history, k) -> recs interface.\n",
        "# Models that are purely item-based just ignore user_id.\n",
        "\n",
        "MODELS_TO_EVAL = {\n",
        "    'popularity':    lambda uid, hist, k: pop_inf.recommend(hist, n=k),\n",
        "    'trending':      lambda uid, hist, k: trend_inf.recommend(hist, n=k),\n",
        "    'cooccurrence':  lambda uid, hist, k: cooc_inf.recommend(hist, n=k),\n",
        "    'item_knn':      lambda uid, hist, k: knn_inf.recommend(hist, top_n=k),\n",
        "    'ials':          lambda uid, hist, k: ials_inf.recommend(uid, hist, top_n=k),\n",
        "}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "K = 10\n",
        "all_results = []\n",
        "\n",
        "for model_name, rec_fn in MODELS_TO_EVAL.items():\n",
        "    t = time.time()\n",
        "    result = evaluate_model(\n",
        "        model_name, rec_fn, EVAL_USERS,\n",
        "        user_history_map, user_future_map, k=K\n",
        "    )\n",
        "    elapsed = time.time() - t\n",
        "    all_results.append(result)\n",
        "    print(\n",
        "        f\"{model_name:<15}  \"\n",
        "        f\"hit@{K}={result['hit@k']:.4f}  \"\n",
        "        f\"ndcg@{K}={result['ndcg@k']:.4f}  \"\n",
        "        f\"prec@{K}={result['precision@k']:.4f}  \"\n",
        "        f\"recall@{K}={result['recall@k']:.4f}  \"\n",
        "        f\"({elapsed:.1f}s, n={result['n_users']:,})\"\n",
        "    )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "popularity       hit@10=0.0140  ndcg@10=0.0027  prec@10=0.0014  recall@10=0.0031  (20.7s, n=2,000)\n",
            "trending         hit@10=0.0020  ndcg@10=0.0004  prec@10=0.0002  recall@10=0.0004  (16.6s, n=2,000)\n",
            "cooccurrence     hit@10=0.0195  ndcg@10=0.0034  prec@10=0.0022  recall@10=0.0038  (119.1s, n=2,000)\n",
            "item_knn         hit@10=0.0115  ndcg@10=0.0020  prec@10=0.0012  recall@10=0.0027  (13.1s, n=2,000)\n",
            "ials             hit@10=0.0160  ndcg@10=0.0025  prec@10=0.0019  recall@10=0.0021  (11.4s, n=2,000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results_df = pd.DataFrame(all_results).set_index('model')\n",
        "display_cols = ['hit@k', 'ndcg@k', 'precision@k', 'recall@k', 'n_users']\n",
        "\n",
        "summary = results_df[display_cols].copy()\n",
        "for col in ['hit@k', 'ndcg@k', 'precision@k', 'recall@k']:\n",
        "    best = summary[col].max()\n",
        "    summary[col] = summary[col].apply(\n",
        "        lambda v: f'{v:.4f} ◀' if v == best else f'{v:.4f}'\n",
        "    )\n",
        "summary['n_users'] = results_df['n_users'].apply(lambda v: f'{v:,}')\n",
        "\n",
        "print(summary.to_string())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Missing optional dependency 'Jinja2'. DataFrame.style requires jinja2. Use pip or conda to install Jinja2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m~/tfod/lib/python3.8/site-packages/pandas/compat/_optional.py:142\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:973\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jinja2'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_results)\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m display_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhit@k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndcg@k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision@k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall@k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_users\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdisplay_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[38;5;241m.\u001b[39mformat({\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhit@k\u001b[39m\u001b[38;5;124m'\u001b[39m:       \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndcg@k\u001b[39m\u001b[38;5;124m'\u001b[39m:      \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision@k\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall@k\u001b[39m\u001b[38;5;124m'\u001b[39m:    \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_users\u001b[39m\u001b[38;5;124m'\u001b[39m:     \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:,}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m })\u001b[38;5;241m.\u001b[39mhighlight_max(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhit@k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndcg@k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision@k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall@k\u001b[39m\u001b[38;5;124m'\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgreen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/tfod/lib/python3.8/site-packages/pandas/core/frame.py:1291\u001b[0m, in \u001b[0;36mDataFrame.style\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstyle\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Styler:\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;124;03m    Returns a Styler object.\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;124;03m        data with HTML and CSS.\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstyle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Styler\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Styler(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[0;32m~/tfod/lib/python3.8/site-packages/pandas/io/formats/style.py:56\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared_docs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _shared_docs\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_to_buffer\n\u001b[0;32m---> 56\u001b[0m jinja2 \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjinja2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataFrame.style requires jinja2.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstyle_render\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     59\u001b[0m     CSSProperties,\n\u001b[1;32m     60\u001b[0m     CSSStyles,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     refactor_levels,\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "File \u001b[0;32m~/tfod/lib/python3.8/site-packages/pandas/compat/_optional.py:145\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'Jinja2'. DataFrame.style requires jinja2. Use pip or conda to install Jinja2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Grouped bar chart — all four metrics side by side per model\n",
        "metrics   = ['hit@k', 'ndcg@k', 'precision@k', 'recall@k']\n",
        "models    = results_df.index.tolist()\n",
        "n_models  = len(models)\n",
        "n_metrics = len(metrics)\n",
        "\n",
        "x = np.arange(n_models)\n",
        "width = 0.18\n",
        "offsets = np.linspace(-(n_metrics - 1) / 2, (n_metrics - 1) / 2, n_metrics) * width\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(11, 5))\n",
        "for i, (metric, offset) in enumerate(zip(metrics, offsets)):\n",
        "    values = results_df[metric].values\n",
        "    bars = ax.bar(x + offset, values, width, label=metric)\n",
        "    for bar, val in zip(bars, values):\n",
        "        ax.text(\n",
        "            bar.get_x() + bar.get_width() / 2,\n",
        "            bar.get_height() + 0.001,\n",
        "            f'{val:.3f}', ha='center', va='bottom', fontsize=7\n",
        "        )\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models, rotation=15, ha='right')\n",
        "ax.set_ylabel(f'Score  (K={K})')\n",
        "ax.set_title(f'Offline Evaluation — All Models @ K={K}  (cutoff: {CUTOFF_TS.date()})')\n",
        "ax.legend(loc='upper right')\n",
        "ax.set_ylim(0, results_df[metrics].max().max() * 1.25)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hit@K across multiple K values — useful for understanding depth of signal\n",
        "K_VALUES = [5, 10, 20, 50]\n",
        "\n",
        "hit_at_k = {}\n",
        "for model_name, rec_fn in MODELS_TO_EVAL.items():\n",
        "    hit_at_k[model_name] = []\n",
        "    for k in K_VALUES:\n",
        "        res = evaluate_model(\n",
        "            model_name, rec_fn, EVAL_USERS,\n",
        "            user_history_map, user_future_map, k=k\n",
        "        )\n",
        "        hit_at_k[model_name].append(res['hit@k'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "for model_name, values in hit_at_k.items():\n",
        "    ax.plot(K_VALUES, values, marker='o', label=model_name)\n",
        "\n",
        "ax.set_xlabel('K')\n",
        "ax.set_ylabel('Hit@K')\n",
        "ax.set_title(f'Hit@K vs K  (cutoff: {CUTOFF_TS.date()})')\n",
        "ax.legend()\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tfodj",
      "language": "python",
      "name": "tfodj"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}